# Decode - High Performance LLMs in JAX

This project implements high-performance Large Language Models (LLMs) in JAX, following the excellent course [HighPerfLLMs2024](https://github.com/rwitten/HighPerfLLMs2024) by Rafi Witten.

## Overview

This implementation focuses on building efficient and performant LLMs from scratch, covering key aspects like:

- Training and inference optimization
- Roofline analysis
- Compilation strategies
- Sharding for distributed computing
- Performance profiling
- Attention mechanisms (including fused attention and flash attention)

## Course Reference

This project follows the curriculum of HighPerfLLMs2024, which covers:

- Building high-performance JAX LLM implementations for both training and inference

## Getting Started

This project uses `uv` for Python package management.

Install `uv`:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

## References

- [HighPerfLLMs2024 Course Repository](https://github.com/rwitten/HighPerfLLMs2024)
